{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import OrderedDict\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import fmin_powell\n",
    "from scipy import integrate\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "from collections import Counter\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "palette = sns.color_palette()\n",
    "figsize = (15,8)\n",
    "legend_fontsize = 16\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif'})\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex',preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "rc('text.latex',preamble=r'\\usepackage[russian]{babel}')\n",
    "rc('axes', **{'titlesize': '16', 'labelsize': '16'})\n",
    "rc('legend', **{'fontsize': '16'})\n",
    "rc('figure', **{'dpi' : 200})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивный байесовский классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('/home/snikolenko/soft/russian_news_corpus/FullWikiNews.csv', 'r', 'cp1251') as f:\n",
    "    all_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('/home/snikolenko/soft/russian_news_corpus/FullWikiNews.csv', encoding='cp1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    words = text.split() # разбиваем текст на слова\n",
    "    res = list()\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize(all_data[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(s):\n",
    "    res, labels = '', []\n",
    "    m, n = re.split(r'\\[\\[[^\\]]*\\]\\]', s), re.findall(r'\\[\\[[^\\]]*\\]\\]', s)\n",
    "    for i,x in enumerate(n):\n",
    "        res += m[i]\n",
    "        if x[2:11] != 'категория':\n",
    "            res += x.split('|')[-1][:-2]\n",
    "        else:\n",
    "            labels.append(x.split(':')[-1][:-2])\n",
    "    res = re.sub(r'\\{\\{[^\\}]*\\}\\}', '', res).strip()\n",
    "    res = re.sub(r'\\|[^|]*\\|', '', res).strip()\n",
    "    res = re.sub(r'\\.', ' ', res).strip()\n",
    "    res = re.sub(r'[*—»«]', '', res).strip()\n",
    "    return res, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = [], []\n",
    "for s in all_data[\"text\"]:\n",
    "    t, ls = preprocess_text(s)\n",
    "    texts.append(t)\n",
    "    labels.append(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clabels = Counter([l for ls in labels for l in ls])\n",
    "print(clabels.most_common(20))\n",
    "\n",
    "classes = ['политика', 'экономика', 'происшествия', 'культура', 'наука и технологии']\n",
    "classes_in_labels = [ np.where([x in ls for x in classes])[0] for ls in labels ]\n",
    "Xtext_full = [x for i,x in enumerate(texts) if len(classes_in_labels[i])>0]\n",
    "y = np.array([np.max(s) for s in classes_in_labels if len(s) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Xtext = []\n",
    "# for i,t in enumerate(Xtext_full):\n",
    "#     if i % 100 == 0:\n",
    "#         print('%d...' % i)\n",
    "#     Xtext.append( lemmatize(t) )\n",
    "\n",
    "# with open('lemtexts.pkl', 'wb') as f:\n",
    "#     pickle.dump(Xtext, f)\n",
    "\n",
    "with open('lemtexts.pkl', 'rb') as f:\n",
    "    Xtext = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "test_set_size = 100\n",
    "test_set = np.sort(np.random.choice(len(y), size=test_set_size))\n",
    "train_set = np.array([ i for i in range(len(y)) if not y in test_set ])\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    return sum(y_pred == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_maxdf = np.arange(0.05, 0.5, 0.05)\n",
    "range_mindf = np.arange(0., 0.02, 0.001)\n",
    "\n",
    "results = {}\n",
    "print(\"min_df\\tmax_df\\tFeatures\\tBernoulliNB\\tMultinomialNB\")\n",
    "for max_df in range_maxdf:\n",
    "    for min_df in range_mindf:\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df)\n",
    "        X_train = vectorizer.fit_transform([ \" \".join(s) for s in np.array(Xtext)[train_set]])\n",
    "        X_test = vectorizer.transform([ \" \".join(s) for s in np.array(Xtext)[test_set]])\n",
    "        y_train, y_test = y[train_set], y[test_set]\n",
    "\n",
    "        model_nbm, model_nbb = MultinomialNB(fit_prior=True), BernoulliNB(fit_prior=True)\n",
    "        model_nbm.fit(X_train, y_train)\n",
    "        model_nbb.fit(X_train, y_train)\n",
    "        acc_nbb, acc_nbm = accuracy(model_nbb.predict(X_test), y_test), accuracy(model_nbm.predict(X_test), y_test)\n",
    "        print(\"%.4f\\t%.4f\\t%d\\t%.4f\\t%.4f\" % (min_df, max_df, len(vectorizer.vocabulary_), acc_nbb, acc_nbm))\n",
    "        results[(min_df, max_df)] = (acc_nbb, acc_nbm, len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_nbb_toplot = np.array([[ results[(min_df, max_df)][0] for min_df in range_mindf ] for max_df in range_maxdf ] )\n",
    "\n",
    "fig = plt.figure(figsize=(15,12))\n",
    "ax = sns.heatmap(res_nbb_toplot, annot=True)\n",
    "ax.set_xticklabels([\"%.3f\" % min_df for min_df in range_mindf ])\n",
    "ax.set_yticklabels([\"%.2f\" % max_df for max_df in range_maxdf ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df, max_df = 0.0050, 0.0500\n",
    "vectorizer = CountVectorizer(min_df=min_df, max_df=max_df)\n",
    "X_train_fulltext = np.array(Xtext_full)[train_set]\n",
    "X_train = vectorizer.fit_transform([ \" \".join(s) for s in np.array(Xtext)[train_set]])\n",
    "\n",
    "N, M, K = X_train.shape[0], X_train.shape[1], len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM-алгоритм для кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(N, M, K)\n",
    "X = X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(X, probs):\n",
    "    z = np.matmul( X, np.log(probs).T )\n",
    "    z = z - np.logaddexp.reduce(z, axis=1).reshape(-1, 1)\n",
    "    return np.exp(z)\n",
    "\n",
    "def m_step(X, z):\n",
    "    probs = ( np.matmul(X.T, z) + np.ones((M, K)) ).T\n",
    "    probs = np.divide( probs, np.sum(probs, axis=1).reshape(-1, 1) )\n",
    "    return probs\n",
    "\n",
    "def log_likelihood(X, z, probs):\n",
    "    return np.sum(np.multiply( z, np.log(np.matmul( X, probs.T )) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.ones((K, M))\n",
    "random_sample = np.random.choice(N, replace=False, size=5*K)\n",
    "for k in range(K):\n",
    "    probs[k] = probs[k] + np.sum(X[random_sample[5*k:5*k+5]], axis=0)\n",
    "probs = np.divide( probs, np.sum(probs, axis=1).reshape(-1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_l = 0\n",
    "for iIter in range(100):\n",
    "    new_z = e_step(X, probs)\n",
    "    new_probs = m_step(X, new_z)\n",
    "    l = log_likelihood(X, new_z, new_probs)\n",
    "    print(\"Итерация %d:\\t\\t%.4f\" % (iIter, l))\n",
    "    if np.abs(l - old_l) < 1e-4:\n",
    "        break\n",
    "    z, probs, old_l = new_z, new_probs, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iCluster in range(5):\n",
    "    print(\"=== Кластер %d ===\\n\" % iCluster)\n",
    "    for iDoc in np.where(z[:, iCluster] > 0.99)[0][:10]:\n",
    "        print(X_train_fulltext[iDoc][:300] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pLSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "Phi, Theta = np.random.random(size=(T, M)), np.random.random(size=(N, T))\n",
    "Phi = np.divide( Phi, np.sum(Phi, axis=1).reshape(-1, 1) )\n",
    "Theta = np.divide( Theta, np.sum(Theta, axis=1).reshape(-1, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(X, Phi, Theta):\n",
    "    n_t, n_dt, n_wt = np.zeros(T), np.zeros((N, T)), np.zeros((M, T))\n",
    "    for i in range(X.shape[0]):\n",
    "        n_tdw = np.divide( np.multiply( np.multiply( Phi, Theta[i].reshape(-1, 1) ), X[i].reshape(1, -1)), np.matmul( Theta[i], Phi ).reshape(1, -1) )\n",
    "        n_dt[i] = np.sum( n_tdw, axis=1 )\n",
    "        n_t += np.sum( n_tdw, axis=1 )\n",
    "        n_wt += n_tdw.T\n",
    "    return n_wt, n_dt, n_t\n",
    "\n",
    "def m_step(X, n_wt, n_dt, n_t):\n",
    "    new_Phi = np.divide( n_wt, np.sum(n_wt, axis=0) ).T\n",
    "    new_Theta = np.divide( n_dt, np.sum(X, axis=1).reshape(-1, 1) )\n",
    "    return new_Phi, new_Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iIter in range(100):\n",
    "    n_wt, n_dt, n_t = e_step(X, Phi, Theta)\n",
    "    new_Phi, new_Theta = m_step(X, n_wt, n_dt, n_t)\n",
    "    \n",
    "    residue = np.sum( (Phi - new_Phi) ** 2 ) + np.sum( (Theta - new_Theta) ** 2 )\n",
    "    print(\"Итерация %d:\\t\\t%.4f\" % (iIter, residue))\n",
    "    if residue < 1e-3:\n",
    "        break\n",
    "    Phi, Theta = new_Phi, new_Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = [x[0] for x in sorted([(w,i) for w,i in vectorizer.vocabulary_.items()], key=lambda x: x[1]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(T):\n",
    "    word_probs = sorted([ (x, Phi[t, i]) for i,x in enumerate(voc) ], key=lambda x: x[1], reverse=True)\n",
    "    print(\"Тема %d\\n%s\\n\\n\" % (t, \"\\n\".join([ \"%20s\\t%.5f\" % (x[0], x[1]) for x in word_probs[:10]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "Phi = np.random.random(size=(M, T))\n",
    "Phi = np.divide( Phi, np.sum(Phi, axis=0).reshape(1, -1) )\n",
    "Theta = np.random.random(size=(N, T))\n",
    "Theta = np.divide( Theta, np.sum(Theta, axis=1).reshape(-1, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = (2./T) * np.ones(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = np.sum(X, axis=1)\n",
    "doc_words = []\n",
    "for x in X:\n",
    "    dw = []\n",
    "    for i in np.where(x > 0)[0]:\n",
    "        for _ in range(x[i]):\n",
    "            dw.append(i)\n",
    "    doc_words.append(np.array(dw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_em(doc_words, lPhi):\n",
    "    Gamma = np.random.random(size=(N, T))\n",
    "    Pi = [ np.zeros((n, T)) for n in doc_lengths ]\n",
    "    for iIter in range(100):\n",
    "        all_digammas = sp.special.digamma(Gamma) - sp.special.digamma(np.sum(Gamma, axis=1)).reshape(-1, 1)\n",
    "        for i,dw in enumerate(doc_words):\n",
    "            Pi[i] = lPhi[doc_words[i]] + all_digammas[i]\n",
    "            Pi[i] = np.exp(Pi[i] - np.logaddexp.reduce(Pi[i], axis=1).reshape(-1, 1))\n",
    "        new_Gamma = np.vstack( [alpha + np.sum(dpi, axis=0) for dpi in Pi] )\n",
    "        residue = np.sum((new_Gamma-Gamma) ** 2)\n",
    "        Gamma = new_Gamma\n",
    "        if residue < 0.01:\n",
    "            break\n",
    "    return Gamma, Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_m_step(doc_words, Pi):\n",
    "    Phi = np.zeros((M, T))\n",
    "    for i,dw in enumerate(doc_words):\n",
    "        Phi[ dw ] += Pi[i]\n",
    "    Phi = np.divide( Phi, np.sum(Phi, axis=0).reshape(1, -1) )\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iIter in range(20):\n",
    "    lPhi = np.log(Phi)\n",
    "    Gamma, Pi = internal_em(doc_words, lPhi)\n",
    "    new_Phi = phi_m_step(doc_words, Pi)\n",
    "    residue = np.sum( (new_Phi - Phi) ** 2)\n",
    "    Phi = new_Phi\n",
    "    print(\"Итерация %d\\t\\t%.7f\" % (iIter, residue))\n",
    "    if residue < 1e-5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(T):\n",
    "    word_probs = sorted([ (x, Phi[i, t]) for i,x in enumerate(voc) ], key=lambda x: x[1], reverse=True)\n",
    "    print(\"Тема %d\\n%s\\n\\n\" % (t, \"\\n\".join([ \"%20s\\t%.5f\" % (x[0], x[1]) for x in word_probs[:10]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = (50./M) * np.ones(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma = np.random.random(size=(N, T))\n",
    "Pi = [ np.zeros((n, T)) for n in doc_lengths ]\n",
    "Lambda = np.random.random(size=(M, T))\n",
    "for iIter in range(200):\n",
    "    Phi = np.divide( Lambda, np.sum(Lambda, axis=0).reshape(1, -1) )\n",
    "    lPhi = np.log(Phi)\n",
    "    all_digammas = sp.special.digamma(Gamma) - sp.special.digamma(np.sum(Gamma, axis=1)).reshape(-1, 1)\n",
    "    for i,dw in enumerate(doc_words):\n",
    "        Pi[i] = lPhi[doc_words[i]] + all_digammas[i]\n",
    "        Pi[i] = np.exp(Pi[i] - np.logaddexp.reduce(Pi[i], axis=1).reshape(-1, 1))\n",
    "    new_Gamma = np.vstack( [alpha + np.sum(dpi, axis=0) for dpi in Pi] )\n",
    "    new_Lambda = np.zeros((M, T)) + beta.reshape(-1, 1)\n",
    "    for i,dw in enumerate(doc_words):\n",
    "        new_Lambda[ dw, : ] += Pi[i]\n",
    "    residue = ( np.sum((new_Gamma-Gamma) ** 2), np.sum((new_Lambda-Lambda) ** 2) )\n",
    "    print(\"Итерация %d\\t\\t%.5f\\t%.5f\" % (iIter, residue[0], residue[1]))\n",
    "    Gamma, Lambda = new_Gamma, new_Lambda\n",
    "    if residue[0] < 1e-2 and residue[1] < 1e-2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = np.divide( Lambda, np.sum(Lambda, axis=0).reshape(1, -1) )\n",
    "for t in range(T):\n",
    "    word_probs = sorted([ (x, Phi[i, t]) for i,x in enumerate(voc) ], key=lambda x: x[1], reverse=True)\n",
    "    print(\"Тема %d\\n%s\\n\\n\" % (t, \"\\n\".join([ \"%20s\\t%.5f\" % (x[0], x[1]) for x in word_probs[:10]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
